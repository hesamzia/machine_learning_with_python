{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4fe09bc-49db-405e-a8c0-b177478c8f1a",
   "metadata": {},
   "source": [
    "# machine_learning_with_python \n",
    "### Regression project\n",
    "##### For this project, we used the King County Housing dataset from Kaggle, which includes homes sold between May 2014 and May 2015, 21 columns, and over 21,000 entries. We intended to use the features of this dataset to estimate price in dollars, but in this experiment, we were not looking for the best formula for this purpose, but rather to show the way of thinking and the path to the model, as well as different regression methods, and also to test some of the features of Python in this context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5cebc0-e5e3-49bb-8368-943357392329",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np     \n",
    "import scipy.stats as stats            # Quantile Quantile plots\n",
    "from scipy.optimize import curve_fit   # For fit data in Non Linear Regression\n",
    "import seaborn as sns  #\n",
    "from sklearn import linear_model       # import linear models from sklearn. Read https://scikit-learn.org/stable/ for learn more\n",
    "from sklearn.metrics import r2_score   # for calculate R2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd70adbc-9621-4c54-bbbc-134beb4e5f60",
   "metadata": {},
   "source": [
    "### Reading the data in\n",
    "#### The data is stored in the file housePrice.csv.The building specifications include the following:\n",
    "#### - id : Unique identifier for a house\n",
    "#### - date : Date house was sold (2014,2015)\n",
    "#### - price : Sale price (prediction target)\n",
    "#### - bedrooms : Number of bedrooms\n",
    "#### - bathrooms : Number of bathrooms\n",
    "#### - sqft_living : Square footage of living space in the home\n",
    "#### - sqft_lot : Square footage of the lot\n",
    "#### - floors : Number of floors (levels) in house\n",
    "#### - waterfront : Whether the house is on a waterfront\n",
    "#### - view : Quality of view from house\n",
    "#### - condition : How good the overall condition of the house is. Related to maintenance of house. See the King County      Assessor Website for further explanation of each condition code\n",
    "#### - grade : Overall grade of the house. Related to the construction and design of the house. See the King County Assessor Website for further explanation of each building grade code\n",
    "#### - sqft_above : Square footage of house apart from basement\n",
    "#### - sqft_basement : Square footage of the basement\n",
    "#### - yr_built : Year when house was built\n",
    "#### - yr_renovated : Year when house was renovated\n",
    "#### - zipcode : ZIP Code used by the United States Postal Service\n",
    "#### - lat : Latitude coordinate\n",
    "#### - long : Longitude coordinate\n",
    "#### - sqft_living15 : The square footage of interior housing living space for the nearest 15 neighbors\n",
    "#### - sqft_lot15 : The square footage of the land lots of the nearest 15 neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9ee29f-3003-4b48-ad8b-d1b079020142",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\".\\data\\kc_house_data.csv\")   # Read cvs from  \\data folder and write in df DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d176a0a-23f5-4384-b742-e2e21b230b58",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "##### - We dropped yr_renovated and waterfront columns from the dataset because they are missing more than 10% of their values. \n",
    "##### - We also dropped id and date columns because their not needed. \n",
    "##### - We needed to remove null values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9316b490-fba4-4ef3-af2b-791775c42c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('yr_renovated', axis=1, inplace=True)  # Drop column\n",
    "df.drop('waterfront', axis=1, inplace=True)    # Drop column\n",
    "df.drop('id', axis=1, inplace=True)            # Drop column\n",
    "df.drop('date', axis=1, inplace=True)          # Drop column\n",
    "df = df.dropna()                               # remove null values\n",
    "df['view'] = df['view'].apply(str)             # Convert to String\n",
    "df['condition'] = df['condition'].apply(str)   # Convert to String\n",
    "df['grade'] = df['grade'].apply(str)           # Convert to String\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb446a28-6612-4630-a2d3-959e0e86fc06",
   "metadata": {},
   "source": [
    "##### We also dropped duplicate rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b25bdff-340b-4ce5-a2d4-3085ecd0f91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = df.drop_duplicates()    #remove duplicate rows\n",
    "df_cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801b1075-a53a-4c48-a8cd-530e16c7022c",
   "metadata": {},
   "source": [
    "### Data Analysis\n",
    "##### Here we were trying to analyze the data using histograms of different variables. As we could see in the histograms below, some of our variables were not normally distributed and the scales were not consistent. Our independent variable “price” had a significant positive skewness that we needed to adjust before modeling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4751a16-158a-4a2b-ac47-41b7c972ea66",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.subplot(4, 3, 1)\n",
    "plt.hist(df['price'],edgecolor=\"red\")\n",
    "plt.title('price')\n",
    "plt.subplot(4, 3, 2)\n",
    "plt.hist(df['bedrooms'],edgecolor=\"red\")\n",
    "plt.title('bedrooms')\n",
    "plt.subplot(4, 3, 3)\n",
    "plt.hist(df['bathrooms'],edgecolor=\"red\")\n",
    "plt.title('bathrooms')\n",
    "plt.subplot(4, 3, 4)\n",
    "plt.hist(df['sqft_living'],edgecolor=\"red\")\n",
    "plt.title('sqft_living')\n",
    "plt.subplot(4, 3, 5)\n",
    "plt.hist(df['sqft_lot'],edgecolor=\"red\")\n",
    "plt.title('sqft_lot')\n",
    "plt.subplot(4, 3, 6)\n",
    "plt.hist(df['floors'],edgecolor=\"red\")\n",
    "plt.title('floors')\n",
    "plt.subplot(4, 3, 7)\n",
    "plt.hist(df['sqft_above'],edgecolor=\"red\")\n",
    "plt.title('sqft_above')\n",
    "plt.subplot(4, 3, 8)\n",
    "plt.hist(df['sqft_basement'],edgecolor=\"red\")\n",
    "plt.title('sqft_basement')\n",
    "plt.subplot(4, 3, 9)\n",
    "plt.hist(df['yr_built'],edgecolor=\"red\")\n",
    "plt.title('yr_built')\n",
    "plt.subplot(4, 3, 10)\n",
    "plt.hist(df['sqft_living15'],edgecolor=\"red\")\n",
    "plt.title('sqft_living15')\n",
    "plt.subplot(4, 3, 11)\n",
    "plt.hist(df['sqft_lot15'],edgecolor=\"red\")\n",
    "plt.title('sqft_lot15')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bb24cd-b1d5-4ba2-973e-4a4ab4e92be6",
   "metadata": {},
   "source": [
    "##### Our categorical variables (view, status, and score), as we could see from the corresponding histograms, had a strong linear relationship with price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86257b6f-bc1a-4c8f-a627-678fccfe9d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 3))\n",
    "plt.subplot(1, 3, 1)\n",
    "df['view'].value_counts().plot(kind='bar')\n",
    "plt.subplot(1, 3, 2)\n",
    "df['condition'].value_counts().plot(kind='bar')\n",
    "plt.subplot(1, 3, 3)\n",
    "df['grade'].value_counts().plot(kind='bar')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b43c54a-69e4-4e72-b15e-1203b4dba118",
   "metadata": {},
   "source": [
    "#### Dummy Encoding\n",
    "##### In this model, it seemed that categorical variables should be coded as dummy variables. This meant that each category should be converted into a new column and assigned the number 1 or 0, depending on the original column.\n",
    "##### We used the pandas function get_dummies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcc0f02-6f1e-44c5-aaba-7bca18176554",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.get_dummies(df)\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecd9906-2302-45ec-8b8d-7f05e519a680",
   "metadata": {},
   "source": [
    "##### We created a heatmap of the correlations to see which variables had the highest correlation with our target variable, price. This was also done to check for multicollinearity of the features. This means that we will only use one of the variables (other than price) that has a high correlation in the modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2487e13-eb4c-4d2f-ba73-062057ea1494",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(40, 40))\n",
    "sns.heatmap(df.corr(), annot=True, cmap='coolwarm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe16941-d31e-4882-9324-61aeecb103a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating list of features and correlations \n",
    "corr = df.corr()\n",
    "features = []\n",
    "correlations = []\n",
    "for idx, correlation in corr['price'].items():\n",
    "    if correlation >= .30 and idx != 'price':\n",
    "        features.append(idx)\n",
    "        correlations.append(correlation)\n",
    "corr_price_df = pd.DataFrame({'Correlations':correlations, 'Features': features}).sort_values(by=['Correlations'], ascending=False)\n",
    "\n",
    "#checking for multicollinearity\n",
    "MC_Features = []\n",
    "MC_Corr = []\n",
    "def check_MC(feature):\n",
    "    for idx, correlation in corr[feature].T.items():\n",
    "        if correlation >= .74 and idx != feature:\n",
    "            MC_Features.append([feature, idx])\n",
    "            MC_Corr.append(correlation)\n",
    "            \n",
    "for feature in corr:\n",
    "    check_MC(feature)\n",
    "MC_df = pd.DataFrame({'Correlations':MC_Corr, 'Features': MC_Features}).sort_values(by=['Correlations'], ascending=False)\n",
    "\n",
    "#printing variable\n",
    "print('Correlations with Price')\n",
    "display(corr_price_df)\n",
    "print('Multicollinear Features')\n",
    "display(MC_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250b3b5f-5244-423f-90a1-019529ae5f48",
   "metadata": {},
   "source": [
    "### We also had to check that the variables used for modeling met the assumptions of linear regression.\n",
    "#### These assumptions are:\n",
    "#### -The variable must have a linear relationship with the price.\n",
    "#### -The variable should be homoscedastic meaning that an equal amount of variance should be seen around the regression line.\n",
    "#### -There should be a normal distribution.\n",
    "##### To examine homoscedastiscity in the most correlated variables and normal distribution, we can use histograms (above) and pairwise plots (below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17063293-8a38-4d82-a13a-4833c4f0c808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using a pairplot \n",
    "sns.set()\n",
    "cols = ['price','sqft_living', 'sqft_above', 'sqft_living15', 'bedrooms']     # Columns with most Correlations with Price\n",
    "sns.pairplot(df[cols], height = 2.5)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9659887-4247-4295-b563-e36ef847a187",
   "metadata": {},
   "source": [
    "##### The variables Sqft_living, sqft_living15, and sqft_above all showed a funnel-like shape in the pairwise plot, so they did not satisfy the assumptions of homogeneity of variance, and we had to transform these variables before modeling. The QQ charts above also show that the price column and the other columns are not normal. So we have reached this conclusion in three different experiments and charts.\n",
    "#####This columns needed to be log/sqrt/cbrt transformed so that they could meet the assumptions for linear regression. After that, we saw if the three different experiments and charts show better conditions or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676779af-4380-4025-bb62-9b9d8fe6390c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['log_price'] = np.log2(df['price'])   # Use price's log\n",
    "plt.figure(figsize=(15, 2.5))\n",
    "plt.subplot(1, 3, 1)\n",
    "stats.probplot(df['log_price'], dist=\"norm\", plot=plt)     # Draw Q-Q plot\n",
    "plt.title('Log(price)')\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.hist(df['log_price'],edgecolor=\"red\")\n",
    "plt.title(\"Log(price)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be39d6f-7b70-4748-83e9-3bda882b6a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['log_bedrooms'] = np.cbrt(df['bedrooms'])   # Use bedrooms's cube-root\n",
    "plt.figure(figsize=(15, 2.5))\n",
    "plt.subplot(1, 3, 1)\n",
    "stats.probplot(df['log_bedrooms'], dist=\"norm\", plot=plt)     # Draw Q-Q plot\n",
    "plt.title('cbrt(bedrooms)')\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.hist(df['log_bedrooms'],edgecolor=\"red\")\n",
    "plt.title(\"cbrt(bedrooms)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3409bcb4-0143-4784-925e-159004ae694d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['log_bathrooms'] = np.cbrt(df['bathrooms'])  # Use bedrooms's cube-root\n",
    "plt.figure(figsize=(15, 2.5))\n",
    "plt.subplot(1, 3, 1)\n",
    "stats.probplot(df['log_bathrooms'], dist=\"norm\", plot=plt)     # Draw Q-Q plot\n",
    "plt.title('cbrt(log_bathrooms)')\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.hist(df['log_bathrooms'],edgecolor=\"red\")\n",
    "plt.title(\"cbrt(log_bathrooms)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cec2e2-fbf3-4bdb-9178-442c8af314f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['log_sqft_living'] = np.log2(df['sqft_living'])  # Use sqft_living's log\n",
    "plt.figure(figsize=(15, 2.5))\n",
    "plt.subplot(1, 3, 1)\n",
    "stats.probplot(df['log_sqft_living'], dist=\"norm\", plot=plt)     # Draw Q-Q plot\n",
    "plt.title('Log(sqft_living)')\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.hist(df['log_sqft_living'],edgecolor=\"red\")\n",
    "plt.title(\"Log(sqft_living)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4062b4-fa80-4d87-aaab-5c6c00ace62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['log_sqft_lot'] = np.log2(df['sqft_lot'])  # Use sqft_living's log\n",
    "plt.figure(figsize=(15, 2.5))\n",
    "plt.subplot(1, 3, 1)\n",
    "stats.probplot(df['log_sqft_lot'], dist=\"norm\", plot=plt)     # Draw Q-Q plot\n",
    "plt.title('Log(sqft_lot)')\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.hist(df['log_sqft_lot'],edgecolor=\"red\")\n",
    "plt.title(\"Log(sqft_lot)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e454504-8d13-434a-b82b-e7d9bc4820e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['log_sqft_above'] = np.log2(df['sqft_above'])  # Use sqft_above's log\n",
    "plt.figure(figsize=(15, 2.5))\n",
    "plt.subplot(1, 3, 1)\n",
    "stats.probplot(df['log_sqft_above'], dist=\"norm\", plot=plt)     # Draw Q-Q plot\n",
    "plt.title('Log(sqft_above)')\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.hist(df['log_sqft_above'],edgecolor=\"red\")\n",
    "plt.title(\"Log(sqft_above)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080c1993-6532-467d-be72-96057e992be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['log_sqft_living15'] = np.log2(df['sqft_living15'])  # Use sqft_living15's log\n",
    "plt.figure(figsize=(15, 2.5))\n",
    "plt.subplot(1, 3, 1)\n",
    "stats.probplot(df['log_sqft_living15'], dist=\"norm\", plot=plt)     # Draw Q-Q plot\n",
    "plt.title('Log(sqft_living15)')\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.hist(df['log_sqft_living15'],edgecolor=\"red\")\n",
    "plt.title(\"Log(sqft_living15)\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6aaed8-8d88-46e4-be5b-c4b92ab39277",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['log_sqft_lot15'] = np.log2(df['sqft_lot15'])  # Use sqft_lot15's log\n",
    "plt.figure(figsize=(15, 2.5))\n",
    "plt.subplot(1, 3, 1)\n",
    "stats.probplot(df['log_sqft_lot15'], dist=\"norm\", plot=plt)     # Draw Q-Q plot\n",
    "plt.title('Log(sqft_lot15)')\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.hist(df['log_sqft_lot15'],edgecolor=\"red\")\n",
    "plt.title(\"Log(sqft_lot15)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25215f58-f6d0-4f5f-99db-dc5ef6606c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "cols = ['log_price','log_sqft_living', 'log_sqft_above', 'log_sqft_living15', 'log_bedrooms']\n",
    "sns.pairplot(df[cols], height = 2.5)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839dd34f-a14a-448e-9baa-51558ec70c9e",
   "metadata": {},
   "source": [
    "##### Compared to our previous pairplot, there was no longer a funnel like shape with our transformed variables. This indicated that assumptions for linear regression are met and we could use these variables for modeling.\n",
    "##### Next, we run some different regression methods on the prepared data.\n",
    "### Simple Linear Regression\n",
    "##### We tested simple linear regression for columns log_sqft_living.\n",
    "##### We randomly separated 80% of the data and obtain a model from it, then we tested our model with the remaining 20%, and finally we checked the result using the Residual sum of squares (MSE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0d0ecd-b7d0-40ba-8e01-3df1fd71d388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask for Area\n",
    "msk = np.random.rand(len(df)) < 0.8\n",
    "train = df[msk]     # mask for 80%\n",
    "test = df[~msk]     #mask for 20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9e688f-00ff-4810-82b3-d4278d3c62f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "regr = linear_model.LinearRegression()        # linear model -- Regression\n",
    "train_x = np.asanyarray(train[['log_sqft_living']])      # data to x array\n",
    "train_y = np.asanyarray(train[['log_price']])    # data to y array\n",
    "regr.fit (train_x, train_y)                         # Regression tetas Calculation\n",
    "# The coefficients\n",
    "print ('Coefficients: ', regr.coef_)     # teta0\n",
    "print ('Intercept: ',regr.intercept_)    # teta1\n",
    "\n",
    "plt.scatter(train.log_sqft_living, train.log_price, color='blue') # scatter from train grade and log_price blue color \n",
    "plt.plot(train_x, regr.coef_[0][0]*train_x + regr.intercept_[0], '-r')  # draw a line teta1x + teta0\n",
    "plt.xlabel(\"log_sqft_living\")\n",
    "plt.ylabel(\"log_price\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b2648d-9fd8-43e4-b655-52b1b2bbe12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = np.asanyarray(test[['log_sqft_living']])         # Test array x\n",
    "test_y = np.asanyarray(test[['log_price']])       # Test array y\n",
    "test_y_ = regr.predict(test_x)                       # predicted y \n",
    "\n",
    "print(\"Mean absolute error: %.2f\" % np.mean(np.absolute(test_y_ - test_y)))     \n",
    "print(\"Residual sum of squares (MSE): %.2f\" % np.mean((test_y_ - test_y) ** 2))\n",
    "print(\"R2-score: %.2f\" % r2_score(test_y , test_y_) )     # "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef2e3ab-9ecf-49c8-914c-f885370dd24e",
   "metadata": {},
   "source": [
    "##### It can be seen that the results of Simple Linear Regression are between 45 and less than 50.\n",
    "### Multiple Linear Regression\n",
    "##### In this section, we performed two experiments with different columns and examine the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4715af3d-424e-4998-8081-95910070d2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First experiment\n",
    "regr = linear_model.LinearRegression()\n",
    "x = np.asanyarray(train[['grade_13','log_sqft_living','log_sqft_above']]) # x1,x2,x3\n",
    "y = np.asanyarray(train[['log_price']])\n",
    "regr.fit (x, y)\n",
    "# The coefficients\n",
    "print ('Coefficients: ', regr.coef_)   # teta matrix\n",
    "print ('Intercept:', regr.intercept_)  # teta0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9348ec1f-ec5f-4f71-be83-a061433b8043",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.asanyarray(test[['grade_13','log_sqft_living','log_sqft_above']]) # xs  3 x with high correlation\n",
    "y = np.asanyarray(test[['log_price']])    # ys\n",
    "y_hat= regr.predict(x)            # predict ys\n",
    "\n",
    "print(\"Residual sum of squares: %.2f\"\n",
    "      % np.mean((y_hat - y) ** 2))\n",
    "\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print('Variance score: %.2f' % regr.score(x, y))      #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91567e1-3589-47f5-a41a-6f79fd144e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seconde experiment\n",
    "regr = linear_model.LinearRegression()\n",
    "x = np.asanyarray(train[['log_sqft_living15','log_sqft_living', 'log_bedrooms', 'zipcode', 'view_0', 'view_4','grade_7','grade_10', 'grade_11']]) # x1,x2,x3\n",
    "y = np.asanyarray(train[['log_price']])\n",
    "regr.fit (x, y)\n",
    "# The coefficients\n",
    "print ('Coefficients: ', regr.coef_)   # teta matrix\n",
    "print ('Intercept:', regr.intercept_)  # teta0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e43a379-ddd3-4731-9ef8-aff7cbb2c298",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.asanyarray(test[['log_sqft_living15','log_sqft_living', 'log_bedrooms', 'zipcode', 'view_0', 'view_4','grade_7','grade_10', 'grade_11']]) # xs  3 x with low correlation\n",
    "y = np.asanyarray(test[['log_price']])    # ys\n",
    "y_hat= regr.predict(x)            # predict ys\n",
    "\n",
    "print(\"Residual sum of squares: %.2f\"\n",
    "      % np.mean((y_hat - y) ** 2))\n",
    "\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print('Variance score: %.2f' % regr.score(x, y))      # "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f9c000-2ad2-49c2-b81f-4b64b658fe7e",
   "metadata": {},
   "source": [
    "### Non Linear Regression\n",
    "##### To show how nonlinear regression works, we assumed that the histogram of log_sqft_living is close to x**2 and tried to fit this plot to our data. However, we did not get a suitable R-squared result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c599b30-dfb7-4366-af0a-536a0dbe0651",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x, Beta_1, Beta_2):      # The function with which we will fit the data\n",
    "     y = Beta_1 + Beta_2 * np.power(x,2)\n",
    "     return y\n",
    "\n",
    "popt, pcov = curve_fit(sigmoid, train['log_sqft_living'], train['log_price'])\n",
    "#print the final parameters\n",
    "print(\" beta_1 = %f, beta_2 = %f\" % (popt[0], popt[1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe13c39-579a-460d-821a-53f891f6c029",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "x=test['log_sqft_living']\n",
    "plt.figure(figsize=(8,5))\n",
    "y = sigmoid(x, *popt)\n",
    "plt.plot(train['log_sqft_living'], train['log_price'], 'ro', label='data')\n",
    "plt.plot(x,y, linewidth=3.0, label='fit')\n",
    "plt.legend(loc='best')\n",
    "plt.ylabel('log_price')\n",
    "plt.xlabel('log_sqft_living')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9ab410-aae2-4adb-a198-e93df81d95c1",
   "metadata": {},
   "source": [
    "##### RMSE and R-squared calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ade3cbe-f2be-4356-9c08-43802c9f015f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.asanyarray(test[['log_sqft_living']]) # xs \n",
    "y = np.asanyarray(test[['log_price']])    # ys\n",
    "\n",
    "y_hat = sigmoid(test['log_sqft_living'], *popt)\n",
    "\n",
    "absError = y_hat.to_numpy() - y\n",
    "\n",
    "SE = np.square(absError) # squared errors\n",
    "MSE = np.mean(SE) # mean squared errors\n",
    "RMSE = np.sqrt(MSE) # Root Mean Squared Error, RMSE\n",
    "\n",
    "Rsquared = 1.0 - (np.var(absError) / np.var(y))\n",
    "print('RMSE:', RMSE)\n",
    "print('R-squared:', Rsquared)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7655180e-ed9e-4a41-8814-33e2d7a404d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
